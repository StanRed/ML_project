{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cравнение инструментов для идентификации сложных слов немецкого языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Проект подготовили:*\n",
    "*  <b>Ященко Анастасия</b> — идея проекта, код, подготовка стандарта\n",
    "*  <b>Соколова Ирина</b> — код, поиск инструментов\n",
    "*  <b>Краснов Станислав</b> — код, поиск инструментов, оформление"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках научного проекта \"Semantic Brain Map\" возникла задача грамматической разметки текста на немецком языке: в частности, была выявлена необходимость определения каждой лексемы как сложносоставной (то есть состоящей из 2 или более <i>корней</i>, без учёта приставок и суффиксов). Настоящий проект посвящён сравнению двух самых популярных инструментов для обработки сложных слов — CharSplit и Compound Word Splitter. Результат их работы сравнивается с подготовленным списком всех сложных лемм, входящих в текст.\n",
    "\n",
    "Мы токенизировали текст и лемматизировали его, используя библиотеку SpaCy (все слова теперь в файле lemmas.txt), затем этот список был предложен носителю языка для того, чтобы в файле checking_lst остались только сложные леммы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2622\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "f1 = open('lemmas.txt')\n",
    "f2 = open('checking_lst.txt')\n",
    "f1 = f1.readlines()\n",
    "f2 = f2.readlines()\n",
    "l = [] #список всех лемм из файла\n",
    "checking_lst = [] #проврочный список\n",
    "for line in f1:\n",
    "    line = re.sub(r'\\n','',line)\n",
    "    l.append(line)\n",
    "for line in f2:\n",
    "    line = re.sub(r'\\n','',line)\n",
    "    checking_lst.append(line)\n",
    "    \n",
    "lemmas = set(l)  #тут все уникальные леммы\n",
    "print(len(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(all_words, results, correct_answers):\n",
    "    tp = [w for w in all_words if w in results and w in correct_answers]\n",
    "    fp = [w for w in all_words if w in results and w not in correct_answers]\n",
    "    tn = [w for w in all_words if w not in results and w not in correct_answers]\n",
    "    fn = [w for w in all_words if w not in results and w in correct_answers]\n",
    "    precision = len(tp) / (len(tp) + len(fp))\n",
    "    recall = len(tp) / (len(tp) + len(fn))\n",
    "    accuracy = (len(tp) + len(tn)) / (len(tp) + len(tn) + len(fp) + len(fn))\n",
    "    f_measure = 2 * precision * recall / (precision + recall)\n",
    "    print('Precision: %.2f' % precision)\n",
    "    print('Recall: %.2f' % recall)\n",
    "    print('Accuracy: %.2f' % accuracy)\n",
    "    print('F-measure: %.2f' % f_measure)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сплиттеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сплиттер compound-word-splitter (CWS) — это подмодуль спеллчекера Enchant. CWS разделяет слова, которые не смог распознать Enchant, на все возможные комбинации двух подстрок, возвращая лишь те из них, что есть в словаре Enchant. Кроме того, CWS учитывает тот факт, что некоторые сложносоставные слова в немецком языке соединяются с помощью согласного <i>-s-</i>. Если CWS не находит в слове подстрок из словаря Enchant, то просто возвращает пустую строку. Очевидно, что с незнакомыми (словарю Enchant'а) словами этот сплиттер работает плохо (никак).\n",
    "\n",
    "CWS сам по себе — довольно удобный и полезный инструмент, который хорошо делит, например, слова с приставками, образованные от тех основ, что есть в словаре. Однако, как показала практика, слова с несколькими основами таким алгоритмом делятся гораздо хуже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import splitter\n",
    "compound_word_splitter = [] #список результатов\n",
    "for lemma in lemmas:\n",
    "    x = splitter.split(lemma, 'de_de')\n",
    "    if x != '': \n",
    "        #поскольку сплиттер возвращает пустую строку в случае, если слово не является сложным (по enchant'у),\n",
    "        #то мы просто добавляем в результирующий список все леммы, которые вызвали НЕ пустую строку\n",
    "        compound_word_splitter.append(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сплиттер char_split находит самые вероятные варианты разбиения сложного слова на составные части. Char_split вычисляет вероятность появления нграммы в начале, в середине и в конце слова и таким образом находит место в слове, которое с наибольшей вероятностью будет стыком двух корней: это место, где низка вероятность середины слова, перед которым идёт нграмма с высокой вероятностью конца слова, а после которого идёт нграмма с высокой вероятностью начала слова. \n",
    "Сначала метод извлекает нграммы длиной от 4 до n в начале слова, в середине слова и в конце. Получив нграммы и вероятности их появления в разных позициях, считается условная вероятность позиции при появлении конкретной нграммы. \n",
    "Затем окно размера от 4 до n, где n равно длина слова минус 3 (минимальная длина части составного слова), перемещается по слову; для каждой позиции в слове вычисляется вероятность того, что здесь может быть стык. Вероятность вычисляется по формуле:\n",
    "score(n) = max p(prefix) + max p(suffix) − min p(infix)\n",
    " \n",
    " Модель обучена на 10 миллионах существительных из газет и достигает accuracy примерно 95%. Надо заметить, что char_split был создан, чтобы выделять в сложных словах вершину в случаях, когда сложное слово целиком не представлено в словаре. Успехом считалось, если сплиттер правильно выделял вершину в слове, а что осталось слева, было неважно. При таких критериях модель работает лучше, чем сплиттеры, основанные на правилах. Однако в тестовом множестве все слова были сложными, поэтому неизвестно, как хорошо этот метод находил бы сложные слова среди разных слов. \n",
    "\n",
    "Как мы использовали char_split:\n",
    "char_split возвращает список вариантов разбиения слова с их вероятностями. У каждого слова разное количество вариантов разбиения, и мы считаем, что чем больше вариантов, тем больше вероятность, что слово сложное. Если вариантов больше пяти, мы добавляем лемму в наш список. *(Сначала пробовали по 4 варианта, затем эмпирическим путём пришли к 5-ти.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import char_split\n",
    "char_split_splitter = [] #список результатов\n",
    "for lemma in lemmas:\n",
    "    x = char_split.split_compound(lemma)\n",
    "    if len(x) > 5:\n",
    "        #этот сплиттер возвращает список списков корней с вероятностями. Чем слово ближе к сложносоставному, \n",
    "        #тем больше вариантов разбиения на основы у него есть. Опытным путём мы решили, что если есть \n",
    "        #больше 5 вариантов разбить слово, то оно, скорее всего, сложное (и мы добавляем лемму в наш список).\n",
    "        char_split_splitter.append(str(lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим на оценку работы сплиттеров (выдача алгоритмов сохранена в отдельных файлах *compound_word_splitter_file.txt*, *char_splitter_file_4.txt* и *char_splitter_file_5.txt*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995\n"
     ]
    }
   ],
   "source": [
    "with open('compound_word_splitter_file.txt') as f:\n",
    "    compound = f.read().split('\\n')\n",
    "\n",
    "print(len(compound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605\n"
     ]
    }
   ],
   "source": [
    "with open('char_splitter_file_4.txt') as f:\n",
    "    char_4 = f.read().split('\\n')\n",
    "print(len(char_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411\n"
     ]
    }
   ],
   "source": [
    "with open('char_splitter_file_5.txt') as f:\n",
    "    char_5 = f.read().split('\\n')\n",
    "print(len(char_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты Compound Word Splitter\n",
      "Precision: 0.15\n",
      "Recall: 0.69\n",
      "Accuracy: 0.30\n",
      "F-measure: 0.25\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Результаты Compound Word Splitter\")\n",
    "print(evaluate(lemmas, compound, checking_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты CharSplit (больше 4 вариантов)\n",
      "Precision: 0.66\n",
      "Recall: 0.89\n",
      "Accuracy: 0.90\n",
      "F-measure: 0.76\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Результаты CharSplit (больше 4 вариантов)\")\n",
    "print(evaluate(lemmas, char_4, checking_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты CharSplit (больше 5 вариантов)\n",
      "Precision: 0.80\n",
      "Recall: 0.73\n",
      "Accuracy: 0.92\n",
      "F-measure: 0.76\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Результаты CharSplit (больше 5 вариантов)\")\n",
    "print(evaluate(lemmas, char_5, checking_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из результатов, CharSplit даёт огромную фору CWS, демонстрируя F-меру в три раза больше, чем у своего \"конкурента\" (0.76 против 0.25). При этом, строго говоря, увеличение в CharSplit кол-ва вариантов для леммы с 4 до 5 не привело к каким-либо значительным качественным изменениям (хотя и уменьшило размер выдачи на треть). Дальнейшее увеличение кол-ва вариантов не приводит к положительным результатам, а лишь уменьшает оценку."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
